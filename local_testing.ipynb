{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class multi_head_kron(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, l_in, l_out, heads, layer_num = 0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.mat1 = nn.Linear(dim_in, heads * dim_out, bias = False)\n",
    "        self.mat1.weight = nn.Parameter(torch.nn.init.uniform_(torch.randn(heads * dim_in, dim_out), a = -(3**0.5), b = 3**0.5) * ((2 ** 0.25) / (dim_in * (heads ** 0.5)) ** 0.5))\n",
    "        self.mat2 = nn.Parameter(torch.nn.init.uniform_(torch.randn(heads,l_in, l_out), a = -(3**0.5), b = 3**0.5) * ((2 ** 0.25) / (l_in * (heads ** 0.5)) ** 0.5))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bias = nn.Parameter(torch.zeros(l_out, dim_out))\n",
    "        # self.bn = nn.BatchNorm1d(l_out)\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'incoming var at layer {self.layer_num}: {torch.var(x)}')\n",
    "        x = self.mat1(x)\n",
    "        x = rearrange(x, 'b l (h d) -> b h l d', h = self.heads)\n",
    "        x = torch.matmul(self.mat2, x)\n",
    "        x = torch.sum(x, dim = 1)\n",
    "        x = x + self.bias\n",
    "        # x = self.bn(x)\n",
    "        print(f'pre activation var at layer {self.layer_num}: {torch.var(x)}')\n",
    "        x = self.activation(x)\n",
    "        print(f'outgoing var at layer {self.layer_num}:  {torch.var(x)}')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0081)\n",
      "incoming var at layer 0: 1.0080957412719727\n",
      "pre activation var at layer 0: 2.0010414123535156\n",
      "outgoing var at layer 0:  0.6822100877761841\n",
      "tensor(0.6822, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "x = torch.randn(3, 100, 100)\n",
    "\n",
    "model = multi_head_kron(100, 100, 100, 100, 8)\n",
    "\n",
    "print(torch.var(x))\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(torch.var(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
