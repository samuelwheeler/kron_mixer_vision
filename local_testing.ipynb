{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class multi_head_kron(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, l_in, l_out, heads, layer_num = 0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.mat1 = nn.Linear(dim_in, heads * dim_out, bias = False)\n",
    "        self.mat1.weight = nn.Parameter(torch.nn.init.uniform_(torch.randn(heads * dim_in, dim_out), a = -(3**0.5), b = 3**0.5) * ((2 ** 0.4) / (dim_in * (heads ** 0.5)) ** 0.5))\n",
    "        self.mat2 = nn.Parameter(torch.nn.init.uniform_(torch.randn(heads,l_in, l_out), a = -(3**0.5), b = 3**0.5) * ((2 ** 0.4) / (l_in * (heads ** 0.5)) ** 0.5))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bias = nn.Parameter(torch.zeros(l_out, dim_out))\n",
    "        # self.bn = nn.BatchNorm1d(l_out)\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'incoming var at layer {self.layer_num}: {torch.var(x)}')\n",
    "        x = self.mat1(x)\n",
    "        x = rearrange(x, 'b l (h d) -> b h l d', h = self.heads)\n",
    "        x = torch.matmul(self.mat2, x)\n",
    "        x = torch.sum(x, dim = 1)\n",
    "        x = x + self.bias\n",
    "        # x = self.bn(x)\n",
    "        print(f'pre activation var at layer {self.layer_num}: {torch.var(x)}')\n",
    "        x = self.activation(x)\n",
    "        print(f'outgoing var at layer {self.layer_num}:  {torch.var(x)}')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n",
      "incoming var at layer 0: 0.9999833106994629\n",
      "pre activation var at layer 0: 3.070035219192505\n",
      "outgoing var at layer 0:  1.0589414834976196\n",
      "tensor(1.0589, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "x = torch.randn(3, 100, 100)\n",
    "\n",
    "model = multi_head_kron(100, 100, 100, 100, 8)\n",
    "\n",
    "print(torch.var(x))\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(torch.var(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0151)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3305, grad_fn=<VarBackward0>)\n",
      "tensor(0.1140, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 100, 100)\n",
    "print(torch.var(x))\n",
    "l = nn.Linear(100, 100)\n",
    "r = nn.ReLU()\n",
    "\n",
    "print(torch.var(l(x)))\n",
    "print(torch.var(r(l(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = torch.zeros(1,1,5)\n",
    "cls_tokens = repeat(cls, '() l d -> b l d', b = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(cls_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5340, -0.3835, -1.2868, -0.4672, -0.2817],\n",
      "         [ 1.3504, -0.7269,  0.3176,  1.8474, -0.7700],\n",
      "         [-0.2543, -0.2582, -0.3357,  0.3720,  0.7625],\n",
      "         [-2.4791,  0.3627,  0.4921, -1.5963, -0.8217]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1769, -0.5190, -0.0534, -0.3373, -2.8793],\n",
      "         [-1.8928,  0.4567, -0.4157, -0.7158,  0.8897],\n",
      "         [-0.4947, -0.2192,  0.4749,  0.7171,  0.4025],\n",
      "         [ 1.0574,  0.9918,  0.4095,  1.1486,  0.0139]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0551, -0.3210,  0.4185,  2.0281,  1.2126],\n",
      "         [ 0.2033, -1.0261,  1.8899,  0.1244, -1.1643],\n",
      "         [-0.3432, -0.3824, -0.2204, -0.0763, -1.4141],\n",
      "         [ 1.2047, -1.0015, -0.1798, -0.2705,  1.3075]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((cls_tokens, x), dim=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
